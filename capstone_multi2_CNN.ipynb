{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape:  (32902, 32, 32)\n",
      "Training labels shape:  (32902, 5)\n",
      "Validing dataset shape:  (500, 32, 32)\n",
      "Validing labels shape:  (500, 5)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'svhn_multi_train.pickle'\n",
    "VALID_SIZE = 500\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    train_dataset = dataset['dataset'][VALID_SIZE:]\n",
    "    train_labels = dataset['labels'][VALID_SIZE:, 1:6]\n",
    "    valid_dataset = dataset['dataset'][:VALID_SIZE]\n",
    "    valid_labels = dataset['labels'][:VALID_SIZE, 1:6]\n",
    "    \n",
    "print('Training dataset shape: ', train_dataset.shape)\n",
    "print('Training labels shape: ', train_labels.shape)\n",
    "print('Validing dataset shape: ', valid_dataset.shape)\n",
    "print('Validing labels shape: ', valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  8, 10, 10, 10],\n",
       "       [ 8,  0, 10, 10, 10],\n",
       "       [ 1,  9,  2, 10, 10],\n",
       "       [ 9,  8, 10, 10, 10],\n",
       "       [ 2,  7, 10, 10, 10],\n",
       "       [ 3,  7,  0, 10, 10],\n",
       "       [ 8, 10, 10, 10, 10],\n",
       "       [ 1,  0,  1,  0, 10],\n",
       "       [ 2,  8,  4, 10, 10],\n",
       "       [ 7,  7, 10, 10, 10]], dtype=int8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (32902, 32, 32, 1) (32902, 5)\n",
      "Validation set (500, 32, 32, 1) (500, 5)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 32\n",
    "NUM_DIGITS = 5\n",
    "NUM_LABELS = 11 # 0-9 + 10==doesn't exist\n",
    "NUM_CHANNELS = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype(np.float32)\n",
    "    #labels = (np.array([10,1,2,3,4,5,6,7,8,9]) == labels).astype(np.float32) # one-hot encoding\n",
    "\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE * 32 * 32 * 1\n",
    "# conv1: 5 * 5 * 1 * 8\n",
    "# BATCH_SIZE * 16 * 16 * 8\n",
    "# conv2: 5 * 5 * 8 * 16\n",
    "# BATCH_SIZE * 8 * 8 * 16\n",
    "# conv3: 5 * 5 * 16 * 32\n",
    "# BATCH_SIZE * 4 * 4 * 32\n",
    "# fc1: 512 * 64\n",
    "# BATCH_SIZE * 64\n",
    "# fc2: 64 * 10 \n",
    "# BATCH_SIZE * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def equal(a):\n",
    "    offset = 6\n",
    "    return a[0] == a[offset] and all(a[i] == a[i+offset] for i in range(1, 6))\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    result = np.apply_along_axis(equal, 1, np.hstack((predictions, labels)))\n",
    "    return tf.reduce_mean(np.cast(result, np.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "PATCH_SIZE = 5\n",
    "DEPTH_1 = 16\n",
    "DEPTH_2 = 32\n",
    "DEPTH_3 = 64\n",
    "NUM_HIDDEN = 64\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86774de98cc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CHANNELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_DIGITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "    tf_train_labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE, NUM_DIGITS))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels, dtype=tf.int64)\n",
    "    \n",
    "    conv1_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, NUM_CHANNELS, DEPTH_1], stddev=0.1))\n",
    "    conv1_biases = tf.Variable(tf.zeros([DEPTH_1]))\n",
    "    conv2_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, DEPTH_1, DEPTH_2], stddev=0.1))\n",
    "    conv2_biases = tf.Variable(tf.zeros([DEPTH_2]))\n",
    "    conv3_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, DEPTH_2, DEPTH_3], stddev=0.1))\n",
    "    conv3_biases = tf.Variable(tf.zeros([DEPTH_3]))\n",
    "    fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE//8 * IMAGE_SIZE//8 * DEPTH_3, NUM_HIDDEN], stddev=0.1))\n",
    "    fc1_biases = tf.Variable(tf.constant(1.0, shape=[NUM_HIDDEN]))\n",
    "    \n",
    "    #fc_numdigit_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, MAX_NUM_DIGIT], stddev=0.1))\n",
    "    #fc_numdigit_biases = tf.Variable(tf.constant(1.0, shape=[MAX_NUM_DIGIT]))\n",
    "    fc_digit1_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit1_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit2_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit2_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit3_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit3_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit4_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit4_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit5_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit5_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    \n",
    "    saver = tf.train.Saver(tf.trainable_variables()) # defaults to saving all variables\n",
    "    \n",
    "    def model(data, train=False):\n",
    "        conv = tf.nn.conv2d(data, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool, conv3_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv3_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1]*shape[2]*shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "        \n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, 0.8, seed=SEED)\n",
    "            \n",
    "        #logit_numdigit = tf.matmul(hidden, fc_numdigit_weights) + fc_numdigit_biases\n",
    "        logit_digit1 = tf.matmul(hidden, fc_digit1_weights) + fc_digit1_biases\n",
    "        logit_digit2 = tf.matmul(hidden, fc_digit2_weights) + fc_digit2_biases\n",
    "        logit_digit3 = tf.matmul(hidden, fc_digit3_weights) + fc_digit3_biases\n",
    "        logit_digit4 = tf.matmul(hidden, fc_digit4_weights) + fc_digit4_biases\n",
    "        logit_digit5 = tf.matmul(hidden, fc_digit5_weights) + fc_digit5_biases\n",
    "        \n",
    "        return logit_digit1, logit_digit2, logit_digit3, logit_digit4, logit_digit5\n",
    "    \n",
    "    def predict(logits):\n",
    "        return tf.transpose(tf.pack([tf.argmax(logits[0], 1), tf.argmax(logits[1], 1), tf.argmax(logits[2], 1), \\\n",
    "                        tf.argmax(logits[3], 1), tf.argmax(logits[4], 1)]))\n",
    "        # return tf.pack([tf.argmax(logits[0], 1), tf.argmax(logits[1], 1), tf.argmax(logits[2], 1), \\\n",
    "                       # tf.argmax(logits[3], 1), tf.argmax(logits[4], 1)], axis=1)\n",
    "    \n",
    "    def accuracy(predictions, labels):\n",
    "        return tf.reduce_mean(tf.cast(tf.reduce_all(tf.equal(predictions, labels), reduction_indices=1), tf.float32)) * 100\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[0], tf_train_labels[:, 0])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[1], tf_train_labels[:, 1])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[2], tf_train_labels[:, 2])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[3], tf_train_labels[:, 3])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[4], tf_train_labels[:, 4]))\n",
    "    \n",
    "    regularizers = tf.nn.l2_loss(fc1_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit1_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit2_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit3_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit4_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit5_weights)\n",
    "                \n",
    "    loss += 5e-4 * regularizers\n",
    "    \n",
    "    batch = tf.Variable(0, dtype=tf.float32)\n",
    "    #decayed_learning_rate = learning_rate *\n",
    "    #                    decay_rate ^ (global_step / decay_steps)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.04, # Base learning rate\n",
    "        batch * BATCH_SIZE, # Current index into the dataset\n",
    "        train_labels.shape[0], # Decay step\n",
    "        0.95, # Decay rate,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "    \n",
    "    train_prediction = predict(logits)\n",
    "    valid_prediction = predict(model(tf_valid_dataset))\n",
    "    \n",
    "    #tf.Print(train_prediction, [train_prediction])\n",
    "    \n",
    "    train_accuracy = accuracy(train_prediction, tf_train_labels)\n",
    "    valid_accuracy = accuracy(valid_prediction, tf_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_in_unison_inplace(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_boolean('training', True, 'If true do the training else load already trained model.')\n",
    "flags.DEFINE_string('checkpoint_dir', 'model/', 'Checkpoint directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Random shuffle\n",
      "Minibatch loss at step 0: 27.109596, learning rate: 0.040000, 1.817s\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 0.2%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "num_steps = 10001\n",
    "\n",
    "FLAGS.training = True\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    if FLAGS.training:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        offset = 0\n",
    "        for step in range(num_steps):\n",
    "            if(offset == 0):\n",
    "                train_dataset, train_labels = shuffle_in_unison_inplace(train_dataset, train_labels)\n",
    "                print('Random shuffle')\n",
    "            feed_dict = {\n",
    "                tf_train_dataset : train_dataset[offset:(offset + BATCH_SIZE)], \n",
    "                tf_train_labels : train_labels[offset:(offset + BATCH_SIZE)]\n",
    "            }\n",
    "            offset += BATCH_SIZE\n",
    "            if offset+BATCH_SIZE > train_labels.shape[0]:\n",
    "                offset = 0\n",
    "            _, l, lr, train_acc, valid_acc = \\\n",
    "                session.run([optimizer, loss, learning_rate, train_accuracy, valid_accuracy], feed_dict=feed_dict)\n",
    "            if (step % 50 == 0):\n",
    "                elapsed_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                print('Minibatch loss at step %d: %f, learning rate: %.6f, %.3fs' % (step, l, lr, elapsed_time))\n",
    "                train_accuracies.append(train_acc)\n",
    "                print('Minibatch accuracy: %.1f%%' % train_accuracies[-1])\n",
    "                valid_accuracies.append(valid_acc)\n",
    "                print('Validation accuracy: %.1f%%' % valid_accuracies[-1])\n",
    "        saver.save(session, 'multi.ckpt', write_meta_graph=False)\n",
    "    else:\n",
    "        saver.restore(session, 'multi.ckpt')\n",
    "    #predictions = test_prediction.eval()\n",
    "    #print('Test accuracy: %.1f%%' % test_accuracy.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
