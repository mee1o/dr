{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape:  (33152, 32, 32)\n",
      "Training labels shape:  (33152, 5)\n",
      "Validing dataset shape:  (250, 32, 32)\n",
      "Validing labels shape:  (250, 5)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'svhn_multi_train.pickle'\n",
    "VALID_SIZE = int(500 / 2)\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    train_dataset = dataset['dataset'][VALID_SIZE:]\n",
    "    train_labels = dataset['labels'][VALID_SIZE:, 1:6]\n",
    "    valid_dataset = dataset['dataset'][:VALID_SIZE]\n",
    "    valid_labels = dataset['labels'][:VALID_SIZE, 1:6]\n",
    "    \n",
    "print('Training dataset shape: ', train_dataset.shape)\n",
    "print('Training labels shape: ', train_labels.shape)\n",
    "print('Validing dataset shape: ', valid_dataset.shape)\n",
    "print('Validing labels shape: ', valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape:  (202902, 32, 32)\n",
      "Training labels shape:  (202902, 5)\n",
      "Validing dataset shape:  (500, 32, 32)\n",
      "Validing labels shape:  (500, 5)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'svhn_multi_extra.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    train_dataset = np.vstack((train_dataset, dataset['dataset'][VALID_SIZE:170000]))\n",
    "    train_labels = np.vstack((train_labels, dataset['labels'][VALID_SIZE:170000, 1:6]))\n",
    "    valid_dataset = np.vstack((valid_dataset, dataset['dataset'][:VALID_SIZE]))\n",
    "    valid_labels = np.vstack((valid_labels, dataset['labels'][:VALID_SIZE, 1:6]))\n",
    "    \n",
    "print('Training dataset shape: ', train_dataset.shape)\n",
    "print('Training labels shape: ', train_labels.shape)\n",
    "print('Validing dataset shape: ', valid_dataset.shape)\n",
    "print('Validing labels shape: ', valid_labels.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  6, 10, 10, 10],\n",
       "       [ 3,  7,  0, 10, 10],\n",
       "       [ 1,  9, 10, 10, 10],\n",
       "       [ 3,  5, 10, 10, 10],\n",
       "       [ 9,  3, 10, 10, 10],\n",
       "       [ 2,  8, 10, 10, 10],\n",
       "       [ 1,  1, 10, 10, 10],\n",
       "       [ 5,  6, 10, 10, 10],\n",
       "       [ 5, 10, 10, 10, 10],\n",
       "       [ 3,  6, 10, 10, 10]], dtype=int8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (202902, 32, 32, 1) (202902, 5)\n",
      "Validation set (500, 32, 32, 1) (500, 5)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 40\n",
    "NUM_DIGITS = 5\n",
    "NUM_LABELS = 11 # 0-9 + 10==doesn't exist\n",
    "NUM_CHANNELS = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype(np.float32)\n",
    "    #labels = (np.array([10,1,2,3,4,5,6,7,8,9]) == labels).astype(np.float32) # one-hot encoding\n",
    "\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# BATCH_SIZE * 32 * 32 * 1\n",
    "# conv1: 5 * 5 * 1 * 8\n",
    "# BATCH_SIZE * 16 * 16 * 8\n",
    "# conv2: 5 * 5 * 8 * 16\n",
    "# BATCH_SIZE * 8 * 8 * 16\n",
    "# conv3: 5 * 5 * 16 * 32\n",
    "# BATCH_SIZE * 4 * 4 * 32\n",
    "# fc1: 512 * 64\n",
    "# BATCH_SIZE * 64\n",
    "# fc2: 64 * 10 \n",
    "# BATCH_SIZE * 10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def equal(a):\n",
    "    offset = 6\n",
    "    return a[0] == a[offset] and all(a[i] == a[i+offset] for i in range(1, 6))\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    result = np.apply_along_axis(equal, 1, np.hstack((predictions, labels)))\n",
    "    return tf.reduce_mean(np.cast(result, np.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "PATCH_SIZE = 5\n",
    "DEPTH_1 = 24\n",
    "DEPTH_2 = 48\n",
    "DEPTH_3 = 80\n",
    "NUM_HIDDEN = 128\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "    tf_train_labels = tf.placeholder(tf.int64, shape=(None, NUM_DIGITS))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels, dtype=tf.int64)\n",
    "    \n",
    "    conv1_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, NUM_CHANNELS, DEPTH_1], stddev=0.1))\n",
    "    conv1_biases = tf.Variable(tf.zeros([DEPTH_1]))\n",
    "    conv2_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, DEPTH_1, DEPTH_2], stddev=0.1))\n",
    "    conv2_biases = tf.Variable(tf.zeros([DEPTH_2]))\n",
    "    conv3_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, DEPTH_2, DEPTH_3], stddev=0.1))\n",
    "    conv3_biases = tf.Variable(tf.zeros([DEPTH_3]))\n",
    "    fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE//8 * IMAGE_SIZE//8 * DEPTH_3, NUM_HIDDEN], stddev=0.1))\n",
    "    fc1_biases = tf.Variable(tf.constant(1.0, shape=[NUM_HIDDEN]))\n",
    "    \n",
    "    #fc_numdigit_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, MAX_NUM_DIGIT], stddev=0.1))\n",
    "    #fc_numdigit_biases = tf.Variable(tf.constant(1.0, shape=[MAX_NUM_DIGIT]))\n",
    "    fc_digit1_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit1_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit2_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit2_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit3_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit3_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit4_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit4_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit5_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit5_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    \n",
    "    saver = tf.train.Saver(tf.trainable_variables()) # defaults to saving all variables\n",
    "    \n",
    "    def model(data, train=False):\n",
    "        conv = tf.nn.conv2d(data, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool, conv3_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv3_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [-1, shape[1]*shape[2]*shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "        \n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, 0.8, seed=SEED)\n",
    "            \n",
    "        #logit_numdigit = tf.matmul(hidden, fc_numdigit_weights) + fc_numdigit_biases\n",
    "        logit_digit1 = tf.matmul(hidden, fc_digit1_weights) + fc_digit1_biases\n",
    "        logit_digit2 = tf.matmul(hidden, fc_digit2_weights) + fc_digit2_biases\n",
    "        logit_digit3 = tf.matmul(hidden, fc_digit3_weights) + fc_digit3_biases\n",
    "        logit_digit4 = tf.matmul(hidden, fc_digit4_weights) + fc_digit4_biases\n",
    "        logit_digit5 = tf.matmul(hidden, fc_digit5_weights) + fc_digit5_biases\n",
    "        \n",
    "        return logit_digit1, logit_digit2, logit_digit3, logit_digit4, logit_digit5\n",
    "    \n",
    "    def predict(logits):\n",
    "        return tf.transpose(tf.pack([tf.argmax(logits[0], 1), tf.argmax(logits[1], 1), tf.argmax(logits[2], 1), \\\n",
    "                        tf.argmax(logits[3], 1), tf.argmax(logits[4], 1)]))\n",
    "        # return tf.pack([tf.argmax(logits[0], 1), tf.argmax(logits[1], 1), tf.argmax(logits[2], 1), \\\n",
    "                       # tf.argmax(logits[3], 1), tf.argmax(logits[4], 1)], axis=1)\n",
    "    \n",
    "    def accuracy(predictions, labels):\n",
    "        return tf.reduce_mean(tf.cast(tf.reduce_all(tf.equal(predictions, labels), reduction_indices=1), tf.float32)) * 100\n",
    "    \n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[0], tf_train_labels[:, 0])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[1], tf_train_labels[:, 1])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[2], tf_train_labels[:, 2])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[3], tf_train_labels[:, 3])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[4], tf_train_labels[:, 4]))\n",
    "    \n",
    "    regularizers = tf.nn.l2_loss(fc1_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit1_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit2_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit3_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit4_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit5_weights)\n",
    "                \n",
    "    loss += 5e-4 * regularizers\n",
    "    \n",
    "    batch = tf.Variable(0, dtype=tf.float32)\n",
    "    #decayed_learning_rate = learning_rate *\n",
    "    #                    decay_rate ^ (global_step / decay_steps)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.04, # Base learning rate\n",
    "        batch * BATCH_SIZE, # Current index into the dataset\n",
    "        train_labels.shape[0], # Decay step\n",
    "        0.95, # Decay rate,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "    \n",
    "    train_prediction = predict(logits)\n",
    "    valid_prediction = predict(model(tf_valid_dataset))\n",
    "    \n",
    "    #tf.Print(train_prediction, [train_prediction])\n",
    "    \n",
    "    train_accuracy = accuracy(train_prediction, tf_train_labels)\n",
    "    valid_accuracy = accuracy(valid_prediction, tf_valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_in_unison_inplace(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_boolean('training', True, 'If true do the training else load already trained model.')\n",
    "flags.DEFINE_string('checkpoint_dir', 'model/', 'Checkpoint directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Random shuffle\n",
      "Minibatch loss at step 0: 38.071728, learning rate: 0.040000, 1.022s\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 1.0%\n",
      "Minibatch loss at step 500: 3.710820, learning rate: 0.040000, 9.659s\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 37.6%\n",
      "Minibatch loss at step 1000: 2.086697, learning rate: 0.040000, 9.647s\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 55.4%\n",
      "Minibatch loss at step 1500: 1.791640, learning rate: 0.040000, 9.650s\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 62.6%\n",
      "Minibatch loss at step 2000: 2.346800, learning rate: 0.040000, 9.660s\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 2500: 1.412897, learning rate: 0.040000, 9.654s\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 3000: 1.429030, learning rate: 0.040000, 9.644s\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 70.0%\n",
      "Random shuffle\n",
      "Minibatch loss at step 3500: 1.742861, learning rate: 0.038000, 9.933s\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 4000: 0.981634, learning rate: 0.038000, 9.652s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 74.0%\n",
      "Minibatch loss at step 4500: 1.534510, learning rate: 0.038000, 9.648s\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 5000: 1.062966, learning rate: 0.038000, 9.654s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 5500: 1.165586, learning rate: 0.038000, 9.641s\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 6000: 1.008470, learning rate: 0.038000, 9.636s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 75.0%\n",
      "Random shuffle\n",
      "Minibatch loss at step 6500: 1.666309, learning rate: 0.036100, 9.936s\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 7000: 0.899586, learning rate: 0.036100, 9.650s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 7500: 1.261969, learning rate: 0.036100, 9.637s\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 8000: 1.080904, learning rate: 0.036100, 9.649s\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 8500: 0.992197, learning rate: 0.036100, 9.639s\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 9000: 0.946420, learning rate: 0.036100, 9.637s\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 9500: 0.899186, learning rate: 0.036100, 9.648s\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 79.8%\n",
      "Random shuffle\n",
      "Minibatch loss at step 10000: 1.500664, learning rate: 0.034295, 9.949s\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 10500: 1.150009, learning rate: 0.034295, 9.693s\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 11000: 0.750028, learning rate: 0.034295, 9.654s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 11500: 0.886230, learning rate: 0.034295, 9.661s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 12000: 0.829292, learning rate: 0.034295, 9.639s\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 12500: 0.870705, learning rate: 0.034295, 9.663s\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 78.8%\n",
      "Random shuffle\n",
      "Minibatch loss at step 13000: 0.806214, learning rate: 0.032580, 9.957s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 13500: 0.616691, learning rate: 0.032580, 9.659s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 14000: 1.270502, learning rate: 0.032580, 9.664s\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 14500: 0.626375, learning rate: 0.032580, 9.654s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 15000: 0.868879, learning rate: 0.032580, 9.657s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 15500: 0.583281, learning rate: 0.032580, 9.653s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 81.2%\n",
      "Random shuffle\n",
      "Minibatch loss at step 16000: 0.482005, learning rate: 0.030951, 9.930s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 16500: 0.910808, learning rate: 0.030951, 9.639s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 17000: 0.470619, learning rate: 0.030951, 9.642s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 17500: 0.535595, learning rate: 0.030951, 9.657s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 18000: 0.663478, learning rate: 0.030951, 9.637s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 18500: 0.639824, learning rate: 0.030951, 9.650s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 19000: 0.727606, learning rate: 0.030951, 9.644s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 82.2%\n",
      "Random shuffle\n",
      "Minibatch loss at step 19500: 0.593493, learning rate: 0.029404, 9.929s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 20000: 0.464140, learning rate: 0.029404, 9.646s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 20500: 0.379481, learning rate: 0.029404, 9.654s\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 21000: 0.920400, learning rate: 0.029404, 9.652s\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 21500: 0.511751, learning rate: 0.029404, 9.645s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 22000: 0.529889, learning rate: 0.029404, 9.645s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.4%\n",
      "Random shuffle\n",
      "Minibatch loss at step 22500: 0.451947, learning rate: 0.027933, 9.938s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 23000: 0.558597, learning rate: 0.027933, 9.639s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 23500: 0.528852, learning rate: 0.027933, 9.661s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 24000: 0.715982, learning rate: 0.027933, 9.643s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 24500: 0.572101, learning rate: 0.027933, 9.649s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 25000: 0.512994, learning rate: 0.027933, 9.644s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.2%\n",
      "Random shuffle\n",
      "Minibatch loss at step 25500: 0.454613, learning rate: 0.026537, 9.933s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 26000: 0.475844, learning rate: 0.026537, 9.656s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 26500: 0.990635, learning rate: 0.026537, 9.657s\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 27000: 0.333784, learning rate: 0.026537, 9.648s\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 83.8%\n",
      "Minibatch loss at step 27500: 0.442046, learning rate: 0.026537, 9.641s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 28000: 0.588346, learning rate: 0.026537, 9.636s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 28500: 0.585509, learning rate: 0.026537, 9.645s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.8%\n",
      "Random shuffle\n",
      "Minibatch loss at step 29000: 0.469461, learning rate: 0.025210, 9.947s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 29500: 0.530830, learning rate: 0.025210, 9.642s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 30000: 0.928863, learning rate: 0.025210, 9.653s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 30500: 0.494083, learning rate: 0.025210, 9.651s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 31000: 0.389175, learning rate: 0.025210, 9.648s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 31500: 0.332726, learning rate: 0.025210, 9.645s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.4%\n",
      "Random shuffle\n",
      "Minibatch loss at step 32000: 0.588939, learning rate: 0.023949, 9.940s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 32500: 0.728139, learning rate: 0.023949, 9.647s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 33000: 0.550108, learning rate: 0.023949, 9.662s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 33500: 0.418498, learning rate: 0.023949, 9.648s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 34000: 0.378153, learning rate: 0.023949, 9.651s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 34500: 0.396604, learning rate: 0.023949, 9.651s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.6%\n",
      "Random shuffle\n",
      "Minibatch loss at step 35000: 0.546736, learning rate: 0.022752, 9.938s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 35500: 0.691966, learning rate: 0.022752, 9.646s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 36000: 0.672847, learning rate: 0.022752, 9.650s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 36500: 0.307424, learning rate: 0.022752, 9.645s\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 37000: 0.912791, learning rate: 0.022752, 9.645s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 37500: 0.325267, learning rate: 0.022752, 9.649s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 38000: 0.519508, learning rate: 0.022752, 9.649s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.4%\n",
      "Random shuffle\n",
      "Minibatch loss at step 38500: 0.536489, learning rate: 0.021614, 9.939s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 39000: 0.291537, learning rate: 0.021614, 9.649s\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 39500: 0.312237, learning rate: 0.021614, 9.653s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 40000: 0.378566, learning rate: 0.021614, 9.648s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 40500: 0.685851, learning rate: 0.021614, 9.654s\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 41000: 0.376931, learning rate: 0.021614, 9.662s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.0%\n",
      "Random shuffle\n",
      "Minibatch loss at step 41500: 0.654910, learning rate: 0.020534, 9.946s\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 42000: 0.395740, learning rate: 0.020534, 9.677s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 42500: 0.367194, learning rate: 0.020534, 9.659s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 43000: 0.229295, learning rate: 0.020534, 9.651s\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 43500: 0.505474, learning rate: 0.020534, 9.647s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 44000: 0.322908, learning rate: 0.020534, 9.652s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.8%\n",
      "Random shuffle\n",
      "Minibatch loss at step 44500: 0.301723, learning rate: 0.019507, 9.936s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 45000: 0.410561, learning rate: 0.019507, 9.645s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 45500: 0.372320, learning rate: 0.019507, 9.647s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 46000: 0.751272, learning rate: 0.019507, 9.646s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 46500: 0.362868, learning rate: 0.019507, 9.648s\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 47000: 0.535759, learning rate: 0.019507, 9.649s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 47500: 0.741186, learning rate: 0.019507, 9.652s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.8%\n",
      "Random shuffle\n",
      "Minibatch loss at step 48000: 0.607246, learning rate: 0.018532, 9.930s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 48500: 0.665272, learning rate: 0.018532, 9.661s\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 49000: 0.542946, learning rate: 0.018532, 9.647s\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 49500: 0.408696, learning rate: 0.018532, 9.647s\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 50000: 0.363366, learning rate: 0.018532, 9.649s\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "NUM_STEPS = 50001\n",
    "SHOW_STATE_AFTER = 500\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    offset = 0\n",
    "    for step in range(NUM_STEPS):\n",
    "        if(offset == 0):\n",
    "            train_dataset, train_labels = shuffle_in_unison_inplace(train_dataset, train_labels)\n",
    "            print('Random shuffle')\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : train_dataset[offset:(offset + BATCH_SIZE)], \n",
    "            tf_train_labels : train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        }\n",
    "        offset += BATCH_SIZE\n",
    "        if offset+BATCH_SIZE > train_labels.shape[0]:\n",
    "            offset = 0\n",
    "        _, l, lr = session.run([optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "        if (step % SHOW_STATE_AFTER == 0):\n",
    "            train_acc, valid_acc = session.run([train_accuracy, valid_accuracy], feed_dict=feed_dict)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            print('Minibatch loss at step %d: %f, learning rate: %.6f, %.3fs' % (step, l, lr, elapsed_time))\n",
    "            train_accuracies.append(train_acc)\n",
    "            print('Minibatch accuracy: %.1f%%' % train_accuracies[-1])\n",
    "            valid_accuracies.append(valid_acc)\n",
    "            print('Validation accuracy: %.1f%%' % valid_accuracies[-1])\n",
    "    saver.save(session, 'multi.ckpt', write_meta_graph=False)\n",
    "    #predictions = test_prediction.eval()\n",
    "    #print('Test accuracy: %.1f%%' % test_accuracy.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "    tf_train_labels = tf.placeholder(tf.int64, shape=(BATCH_SIZE, NUM_DIGITS))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_valid_labels = tf.constant(valid_labels, dtype=tf.int64)\n",
    "    \n",
    "    conv1_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, NUM_CHANNELS, DEPTH_1], stddev=0.1))\n",
    "    conv1_biases = tf.Variable(tf.zeros([DEPTH_1]))\n",
    "    conv2_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, DEPTH_1, DEPTH_2], stddev=0.1))\n",
    "    conv2_biases = tf.Variable(tf.zeros([DEPTH_2]))\n",
    "    conv3_weights = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, DEPTH_2, DEPTH_3], stddev=0.1))\n",
    "    conv3_biases = tf.Variable(tf.zeros([DEPTH_3]))\n",
    "    fc1_weights = tf.Variable(tf.truncated_normal([IMAGE_SIZE//8 * IMAGE_SIZE//8 * DEPTH_3, NUM_HIDDEN], stddev=0.1))\n",
    "    fc1_biases = tf.Variable(tf.constant(1.0, shape=[NUM_HIDDEN]))\n",
    "    \n",
    "    #fc_numdigit_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, MAX_NUM_DIGIT], stddev=0.1))\n",
    "    #fc_numdigit_biases = tf.Variable(tf.constant(1.0, shape=[MAX_NUM_DIGIT]))\n",
    "    fc_digit1_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit1_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit2_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit2_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit3_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit3_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit4_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit4_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    fc_digit5_weights = tf.Variable(tf.truncated_normal([NUM_HIDDEN, NUM_LABELS], stddev=0.1))\n",
    "    fc_digit5_biases = tf.Variable(tf.constant(1.0, shape=[NUM_LABELS]))\n",
    "    \n",
    "    saver = tf.train.Saver(tf.trainable_variables()) # defaults to saving all variables\n",
    "    \n",
    "    def model(data, train=False):\n",
    "        conv = tf.nn.conv2d(data, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pool, conv3_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, conv3_biases))\n",
    "        pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        shape = pool.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool, [shape[0], shape[1]*shape[2]*shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "        \n",
    "        if train:\n",
    "            hidden = tf.nn.dropout(hidden, 0.9, seed=SEED)\n",
    "            \n",
    "        #logit_numdigit = tf.matmul(hidden, fc_numdigit_weights) + fc_numdigit_biases\n",
    "        logit_digit1 = tf.matmul(hidden, fc_digit1_weights) + fc_digit1_biases\n",
    "        logit_digit2 = tf.matmul(hidden, fc_digit2_weights) + fc_digit2_biases\n",
    "        logit_digit3 = tf.matmul(hidden, fc_digit3_weights) + fc_digit3_biases\n",
    "        logit_digit4 = tf.matmul(hidden, fc_digit4_weights) + fc_digit4_biases\n",
    "        logit_digit5 = tf.matmul(hidden, fc_digit5_weights) + fc_digit5_biases\n",
    "        \n",
    "        return logit_digit1, logit_digit2, logit_digit3, logit_digit4, logit_digit5\n",
    "    \n",
    "    def predict(logits):\n",
    "        return tf.transpose(tf.pack([tf.argmax(logits[0], 1), tf.argmax(logits[1], 1), tf.argmax(logits[2], 1), \\\n",
    "                        tf.argmax(logits[3], 1), tf.argmax(logits[4], 1)]))\n",
    "        # return tf.pack([tf.argmax(logits[0], 1), tf.argmax(logits[1], 1), tf.argmax(logits[2], 1), \\\n",
    "                       # tf.argmax(logits[3], 1), tf.argmax(logits[4], 1)], axis=1)\n",
    "    \n",
    "    def accuracy(predictions, labels):\n",
    "        return tf.reduce_mean(tf.cast(tf.reduce_all(tf.equal(predictions, labels), reduction_indices=1), tf.float32)) * 100\n",
    "    \n",
    "    logits = model(tf_train_dataset, True)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[0], tf_train_labels[:, 0])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[1], tf_train_labels[:, 1])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[2], tf_train_labels[:, 2])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[3], tf_train_labels[:, 3])) + \\\n",
    "            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits[4], tf_train_labels[:, 4]))\n",
    "    \n",
    "    regularizers = tf.nn.l2_loss(fc1_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit1_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit2_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit3_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit4_weights) + \\\n",
    "                tf.nn.l2_loss(fc_digit5_weights)\n",
    "                \n",
    "    loss += 5e-4 * regularizers\n",
    "    \n",
    "    batch = tf.Variable(0, dtype=tf.float32)\n",
    "    #decayed_learning_rate = learning_rate *\n",
    "    #                    decay_rate ^ (global_step / decay_steps)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.01, # Base learning rate\n",
    "        batch * BATCH_SIZE, # Current index into the dataset\n",
    "        train_labels.shape[0], # Decay step\n",
    "        0.9, # Decay rate,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "    \n",
    "    train_prediction = predict(logits)\n",
    "    valid_prediction = predict(model(tf_valid_dataset))\n",
    "    \n",
    "    #tf.Print(train_prediction, [train_prediction])\n",
    "    \n",
    "    train_accuracy = accuracy(train_prediction, tf_train_labels)\n",
    "    valid_accuracy = accuracy(valid_prediction, tf_valid_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "num_steps = 10001\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    saver.restore(session, 'multi3.ckpt')\n",
    "    offset = 0\n",
    "    for step in range(num_steps):\n",
    "        if(offset == 0):\n",
    "            train_dataset, train_labels = shuffle_in_unison_inplace(train_dataset, train_labels)\n",
    "            print('Random shuffle')\n",
    "        feed_dict = {\n",
    "            tf_train_dataset : train_dataset[offset:(offset + BATCH_SIZE)], \n",
    "            tf_train_labels : train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        }\n",
    "        offset += BATCH_SIZE\n",
    "        if offset+BATCH_SIZE > train_labels.shape[0]:\n",
    "            offset = 0\n",
    "        _, l, lr, train_acc, valid_acc = \\\n",
    "            session.run([optimizer, loss, learning_rate, train_accuracy, valid_accuracy], feed_dict=feed_dict)\n",
    "        if (step % 100 == 0):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            print('Minibatch loss at step %d: %f, learning rate: %.6f, %.3fs' % (step, l, lr, elapsed_time))\n",
    "            train_accuracies.append(train_acc)\n",
    "            print('Minibatch accuracy: %.1f%%' % train_accuracies[-1])\n",
    "            valid_accuracies.append(valid_acc)\n",
    "            print('Validation accuracy: %.1f%%' % valid_accuracies[-1])\n",
    "    saver.save(session, 'multi4.ckpt', write_meta_graph=False)\n",
    "    #predictions = test_prediction.eval()\n",
    "    #print('Test accuracy: %.1f%%' % test_accuracy.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(0, NUM_STEPS, SHOW_STATE_AFTER), train_accuracies)\n",
    "plt.plot(range(0, NUM_STEPS, SHOW_STATE_AFTER), valid_accuracies)\n",
    "plt.title('Learning curve')\n",
    "plt.xlabel('#Steps')\n",
    "plt.ylabel('Accuracy(%)')\n",
    "plt.legend(['Train accuracy', 'Valid accuracy'], loc='best')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
